# PVC for WIPP-plugins
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wipp-pv-claim
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 20Gi
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: wipp-backend-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: wipp-backend-role
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "watch", "patch"]
  - apiGroups: [""]
    resources: ["pods/log"]
    verbs: ["get", "watch"]
  - apiGroups: ["argoproj.io"]
    resources: ["workflows"]
    verbs: ["delete", "deletecollection", "get", "list", "patch", "create", "update", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: wipp-backend-rb
subjects:
  - kind: ServiceAccount
    name: wipp-backend-sa
roleRef:
  kind: Role
  name: wipp-backend-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wipp-backend
spec:
  selector:
    matchLabels:
      app: wipp-backend
  replicas: 1
  template:
    metadata:
      labels:
        app: wipp-backend
    spec:
      serviceAccountName: wipp-backend-sa
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
      containers:
        - image: wipp/wipp-backend:3.0.0-rc1
          name: wipp-backend
          imagePullPolicy: Always
          args: ["wipp-mongo", "27017", "wipp-pv-claim"]
          env:
            - name: KEYCLOAK_AUTH_URL
              value: "http://localhost:32006/auth"
            - name: KEYCLOAK_SSL_REQUIRED
              value: "external"
            - name: KEYCLOAK_DISABLE_TRUST_MANAGER
              value: "false"
          volumeMounts:
            - mountPath: /data/WIPP-plugins
              name: data
          ports:
            - containerPort: 8080
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: wipp-pv-claim
      restartPolicy: Always
---
# WIPP-Backend Service
apiVersion: v1
kind: Service
metadata:
  name: wipp-backend
spec:
  type: NodePort
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: https
    port: 443
    protocol: TCP
    targetPort: 8080
  selector:
    app: wipp-backend
---
# WIPP Frontend Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wipp-frontend
spec:
  selector:
    matchLabels:
      app: wipp-frontend
  replicas: 1
  template:
    metadata:
      labels:
        app: wipp-frontend
    spec:
      containers:
      - image: wipp/wipp-frontend:3.0.0-rc1
        name: wipp-frontend
        imagePullPolicy: Always
        args: ["wipp-backend", "8080"]
        env:
        - name: ARGOUIBASE_URL
          value: "http://localhost:32002/workflows/default"
        - name: JUPYTERHUB_URL
          value: "http://localhost:32003"
        - name: VISIONUI_URL
          value: "http://localhost:32004"
        - name: TENSORBOARD_URL
          value: "http://localhost:32005"
        - name: KEYCLOAK_URL
          value: "http://localhost:32006/auth"
        ports:
        - containerPort: 80
      restartPolicy: Always
---
# WIPP Frontend Service
apiVersion: v1
kind: Service
metadata:
  name: wipp-frontend
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  - name: https
    port: 443
    nodePort: 32001
    protocol: TCP
    targetPort: 80
  selector:
    app: wipp-frontend
---
# PVC for MongoDB
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongo-pv-claim
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
# WIPP MongoDB Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - image: mongo:3.6
        name: mongodb
        volumeMounts:
        - mountPath: /data/db
          name: dbdata
          subPath: db
        ports:
        - containerPort: 27017
      volumes:
      - name: dbdata
        persistentVolumeClaim:
          claimName: mongo-pv-claim
      restartPolicy: Always
---
# WIPP MongoDB Service
apiVersion: v1
kind: Service
metadata:
  name: wipp-mongo
spec:
  type: NodePort
  ports:
  - name: http
    port: 27017
    protocol: TCP
    targetPort: 27017
  selector:
    app: mongodb
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: postgres
  name: postgres
spec:
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
      name: postgres
    spec:
      containers:
      - env:
        - name: POSTGRES_DB
          value: keycloak
        - name: POSTGRES_PASSWORD
          value: password
        - name: POSTGRES_USER
          value: keycloak
        image: postgres:12
        name: postgres
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: postgres-data
        ports:
        - containerPort: 5432
        resources: {}
      restartPolicy: Always
      volumes:
      - name: postgres-data
        hostPath:
          path: /data/WIPP-postgres
          type: Directory
status: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    service: postgres
  name: postgres
spec:
  ports:
  - name: "5432"
    port: 5432
    protocol: TCP
  selector:
    app: postgres
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wipp-keycloak
  labels:
    app: wipp-keycloak
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wipp-keycloak
  template:
    metadata:
      labels:
        app: wipp-keycloak
    spec:
      containers:
      - args:
        - -b 0.0.0.0 -Dkeycloak.import=/opt/jboss/keycloak/imports/realm.json
        env:
        - name: DB_ADDR
          value: postgres
        - name: DB_DATABASE
          value: keycloak
        - name: DB_PASSWORD
          value: password
        - name: DB_SCHEMA
          value: public
        - name: DB_USER
          value: keycloak
        - name: DB_VENDOR
          value: POSTGRES
        - name: KEYCLOAK_PASSWORD
          value: admin
        - name: KEYCLOAK_USER
          value: admin
        - name:  PROXY_ADDRESS_FORWARDING
          value: "true"
        image: quay.io/keycloak/keycloak:11.0.2
        name: keycloak
        ports:
        - name: http
          containerPort: 8080
        - name: https
          containerPort: 8443
        readinessProbe:
          httpGet:
            path: /auth/realms/master
            port: 8080
        volumeMounts:
        - mountPath: "/opt/jboss/keycloak/imports/"
          name: wipp-realm
          readOnly: true
      restartPolicy: Always
      volumes:
      - name: wipp-realm
        secret:
          secretName: wipp-realm-secret
status: {}
---
apiVersion: v1
kind: Service
metadata:
  name: wipp-keycloak
  labels:
    app: wipp-keycloak
spec:
  type: NodePort
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    nodePort: 32006
  selector:
    app: wipp-keycloak
status:
  loadBalancer: {}
---
apiVersion: v1
kind: Namespace
metadata:
  name: argo
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: workflows.argoproj.io
spec:
  group: argoproj.io
  names:
    kind: Workflow
    plural: workflows
    shortNames:
    - wf
  scope: Namespaced
  version: v1alpha1
---
apiVersion: apiextensions.k8s.io/v1beta1	
kind: CustomResourceDefinition	
metadata:	
  name: workflowtemplates.argoproj.io	
spec:	
  group: argoproj.io	
  names:	
    kind: WorkflowTemplate	
    plural: workflowtemplates	
    shortNames:	
    - wftmpl	
  scope: Namespaced	
  version: v1alpha1	
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: argo
  namespace: argo
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: argo-ui
  namespace: argo
---	
apiVersion: rbac.authorization.k8s.io/v1	
kind: Role	
metadata:	
  name: argo-role
  namespace: argo
rules:	
- apiGroups:	
  - ""	
  resources:	
  - secrets	
  verbs:	
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
  name: argo-aggregate-to-admin
rules:
- apiGroups:
  - argoproj.io
  resources:
  - workflows
  - workflows/finalizers
  - workflowtemplates	
  - workflowtemplates/finalizers
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
  name: argo-aggregate-to-edit
rules:
- apiGroups:
  - argoproj.io
  resources:
  - workflows
  - workflows/finalizers
  - workflowtemplates	
  - workflowtemplates/finalizers
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: argo-aggregate-to-view
rules:
- apiGroups:
  - argoproj.io
  resources:
  - workflows
  - workflows/finalizers
  - workflowtemplates	
  - workflowtemplates/finalizers
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: argo-cluster-role
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - pods/exec
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - create
  - delete
- apiGroups:
  - argoproj.io
  resources:
  - workflows
  - workflows/finalizers
  verbs:
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:	
  - argoproj.io	
  resources:	
  - workflowtemplates	
  - workflowtemplates/finalizers	
  verbs:	
  - get	
  - list	
  - watch	
- apiGroups:	
  - ""	
  resources:	
  - serviceaccounts	
  verbs:	
  - get	
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: argo-ui-cluster-role
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - pods/exec
  - pods/log
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
- apiGroups:
  - argoproj.io
  resources:
  - workflows
  - workflowtemplates
  verbs:
  - get
  - list
  - watch
---	
apiVersion: rbac.authorization.k8s.io/v1	
kind: RoleBinding	
metadata:	
  name: argo-binding	
  namespace: argo
roleRef:	
  apiGroup: rbac.authorization.k8s.io	
  kind: Role	
  name: argo-role	
subjects:	
- kind: ServiceAccount	
  name: argo
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: argo-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: argo-cluster-role
subjects:
- kind: ServiceAccount
  name: argo
  namespace: argo
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: argo-ui-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: argo-ui-cluster-role
subjects:
- kind: ServiceAccount
  name: argo-ui
  namespace: argo
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: workflow-controller-configmap
  namespace: argo
---
apiVersion: v1
kind: Service
metadata:
  name: argo-ui
  namespace: argo
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 32002
    targetPort: 8001
  selector:
    app: argo-ui
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: argo-ui
  namespace: argo
spec:
  selector:
    matchLabels:
      app: argo-ui
  template:
    metadata:
      labels:
        app: argo-ui
    spec:
      containers:
      - env:
        - name: ARGO_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: IN_CLUSTER
          value: "true"
        - name: ENABLE_WEB_CONSOLE
          value: "false"
        - name: BASE_HREF
          value: /
        image: argoproj/argoui:v2.4.3
        name: argo-ui
      serviceAccountName: argo-ui
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workflow-controller
  namespace: argo
spec:
  selector:
    matchLabels:
      app: workflow-controller
  template:
    metadata:
      labels:
        app: workflow-controller
    spec:
      containers:
      - args:
        - --configmap
        - workflow-controller-configmap
        - --executor-image
        - argoproj/argoexec:v2.4.3
        command:
        - workflow-controller
        image: argoproj/workflow-controller:v2.4.3
        name: workflow-controller
      serviceAccountName: argo
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: argo-user-sa
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: argo-user-role
rules:
  - apiGroups: ["argoproj.io"]
    resources: ["workflows"]
    verbs: ["delete", "deletecollection", "get", "list", "patch", "create", "update", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: argo-user-rb
subjects:
  - kind: ServiceAccount
    namespace: default
    name: argo-user-sa
roleRef:
  kind: Role
  name: argo-user-role
  apiGroup: rbac.authorization.k8s.io
---
# JupyterHub spawn config
apiVersion: v1
kind: ConfigMap
metadata:
  name: jupyterhub-config
data:
  jupyterhub-config.py: |
    import os,sys

    c = get_config()

    c.JupyterHub.spawner_class='kubespawner.KubeSpawner'
    c.KubeSpawner.start_timeout=1000

    # Which container to spawn
    c.KubeSpawner.image_spec='labshare/polyglot-notebook:0.5.3'
    c.KubeSpawner.default_url = '/lab'
    c.KubeSpawner.uid = 1000 #uid 1000 corresponds to jovyan, uid 0 to root
    c.KubeSpawner.cmd = ['jupyter-labhub']
    c.KubeSpawner.working_dir = '/home/jovyan'
    c.KubeSpawner.service_account='jupyteruser-sa'

    # Per-user storage configuration
    c.KubeSpawner.pvc_name_template = 'claim-{username}'
    c.KubeSpawner.storage_capacity = '1Gi'
    c.KubeSpawner.storage_access_modes = ['ReadWriteOnce']
    c.KubeSpawner.storage_pvc_ensure = True

    # Volumes to attach to Pod
    c.KubeSpawner.volumes = [
      {
        'name': 'volume-{username}',
        'persistentVolumeClaim': {
          'claimName': 'claim-{username}'
        }
      },
      {
        'name': 'shared-volume',
        'persistentVolumeClaim': {
          'claimName': 'notebooks-pv-claim'
        }
      },
      {
        'name': 'wipp-volume',
        'persistentVolumeClaim': {
          'claimName': 'wipp-pv-claim'
        }
      }
    ]

    # Where to mount volumes
    c.KubeSpawner.volume_mounts = [
      {
        'mountPath': '/home/jovyan/work',
        'name': 'volume-{username}'
      },
      {
        'mountPath': '/opt/shared/notebooks',
        'name': 'shared-volume'
      },
      {
        'mountPath': '/opt/shared/wipp',
        'name': 'wipp-volume'
      }
    ]

    c.JupyterHub.allow_named_servers=True
    c.JupyterHub.ip='0.0.0.0'
    c.JupyterHub.hub_ip='0.0.0.0'

    # Required for AWS
    c.JupyterHub.hub_connect_ip='jupyterhub-internal'

    c.JupyterHub.cleanup_servers=False
    # c.ConfigurableHTTPProxy.should_start=False
    c.JupyterHub.cookie_secret_file = '/srv/jupyterhub/jupyterhub_cookie_secret'

    c.JupyterHub.authenticator_class = 'dummy'

    # Set up WIPP UI url for integration with WIPP
    c.KubeSpawner.environment = {
        'WIPP_UI_URL': 'http://localhost:32001',
        'WIPP_API_INTERNAL_URL': 'http://wipp-backend:8080/api',
        'WIPP_NOTEBOOKS_PATH': '/opt/shared/wipp/temp/notebooks'
    }

    # Service to shutdown inactive Notebook servers after --timeout seconds
    c.JupyterHub.services = [
        {
            'name': 'cull-idle',
            'admin': True,
            'command': [sys.executable, '/srv/jupyterhub/config/cull-idle-servers.py', '--timeout=36000'],
        }
    ]
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cull-idle-servers
data:
  cull-idle-servers.py: |
    import json
    import os
    from datetime import datetime
    from datetime import timezone
    from functools import partial

    try:
        from urllib.parse import quote
    except ImportError:
        from urllib import quote

    import dateutil.parser

    from tornado.gen import coroutine, multi
    from tornado.locks import Semaphore
    from tornado.log import app_log
    from tornado.httpclient import AsyncHTTPClient, HTTPRequest
    from tornado.ioloop import IOLoop, PeriodicCallback
    from tornado.options import define, options, parse_command_line


    def parse_date(date_string):
        """Parse a timestamp
        If it doesn't have a timezone, assume utc
        Returned datetime object will always be timezone-aware
        """
        dt = dateutil.parser.parse(date_string)
        if not dt.tzinfo:
            # assume na√Øve timestamps are UTC
            dt = dt.replace(tzinfo=timezone.utc)
        return dt


    def format_td(td):
        """
        Nicely format a timedelta object
        as HH:MM:SS
        """
        if td is None:
            return "unknown"
        if isinstance(td, str):
            return td
        seconds = int(td.total_seconds())
        h = seconds // 3600
        seconds = seconds % 3600
        m = seconds // 60
        seconds = seconds % 60
        return "{h:02}:{m:02}:{seconds:02}".format(h=h, m=m, seconds=seconds)


    @coroutine
    def cull_idle(
        url, api_token, inactive_limit, cull_users=False, max_age=0, concurrency=10
    ):
        """Shutdown idle single-user servers
        If cull_users, inactive *users* will be deleted as well.
        """
        auth_header = {'Authorization': 'token %s' % api_token}
        req = HTTPRequest(url=url + '/users', headers=auth_header)
        now = datetime.now(timezone.utc)
        client = AsyncHTTPClient()

        if concurrency:
            semaphore = Semaphore(concurrency)

            @coroutine
            def fetch(req):
                """client.fetch wrapped in a semaphore to limit concurrency"""
                yield semaphore.acquire()
                try:
                    return (yield client.fetch(req))
                finally:
                    yield semaphore.release()

        else:
            fetch = client.fetch

        resp = yield fetch(req)
        users = json.loads(resp.body.decode('utf8', 'replace'))
        futures = []

        @coroutine
        def handle_server(user, server_name, server):
            """Handle (maybe) culling a single server
            Returns True if server is now stopped (user removable),
            False otherwise.
            """
            log_name = user['name']
            if server_name:
                log_name = '%s/%s' % (user['name'], server_name)
            if server.get('pending'):
                app_log.warning(
                    "Not culling server %s with pending %s", log_name, server['pending']
                )
                return False

            # jupyterhub < 0.9 defined 'server.url' once the server was ready
            # as an *implicit* signal that the server was ready.
            # 0.9 adds a dedicated, explicit 'ready' field.
            # By current (0.9) definitions, servers that have no pending
            # events and are not ready shouldn't be in the model,
            # but let's check just to be safe.

            if not server.get('ready', bool(server['url'])):
                app_log.warning(
                    "Not culling not-ready not-pending server %s: %s", log_name, server
                )
                return False

            if server.get('started'):
                age = now - parse_date(server['started'])
            else:
                # started may be undefined on jupyterhub < 0.9
                age = None

            # check last activity
            # last_activity can be None in 0.9
            if server['last_activity']:
                inactive = now - parse_date(server['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'started' field which is never None
                # for running servers
                inactive = age

            should_cull = (
                inactive is not None and inactive.total_seconds() >= inactive_limit
            )
            if should_cull:
                app_log.info(
                    "Culling server %s (inactive for %s)", log_name, format_td(inactive)
                )

            if max_age and not should_cull:
                # only check started if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling server %s (age: %s, inactive for %s)",
                        log_name,
                        format_td(age),
                        format_td(inactive),
                    )
                    should_cull = True

            if not should_cull:
                app_log.debug(
                    "Not culling server %s (age: %s, inactive for %s)",
                    log_name,
                    format_td(age),
                    format_td(inactive),
                )
                return False

            if server_name:
                # culling a named server
                delete_url = url + "/users/%s/servers/%s" % (
                    quote(user['name']),
                    quote(server['name']),
                )
            else:
                delete_url = url + '/users/%s/server' % quote(user['name'])

            req = HTTPRequest(url=delete_url, method='DELETE', headers=auth_header)
            resp = yield fetch(req)
            if resp.code == 202:
                app_log.warning("Server %s is slow to stop", log_name)
                # return False to prevent culling user with pending shutdowns
                return False
            return True

        @coroutine
        def handle_user(user):
            """Handle one user.
            Create a list of their servers, and async exec them.  Wait for
            that to be done, and if all servers are stopped, possibly cull
            the user.
            """
            # shutdown servers first.
            # Hub doesn't allow deleting users with running servers.
            # jupyterhub 0.9 always provides a 'servers' model.
            # 0.8 only does this when named servers are enabled.
            if 'servers' in user:
                servers = user['servers']
            else:
                # jupyterhub < 0.9 without named servers enabled.
                # create servers dict with one entry for the default server
                # from the user model.
                # only if the server is running.
                servers = {}
                if user['server']:
                    servers[''] = {
                        'last_activity': user['last_activity'],
                        'pending': user['pending'],
                        'url': user['server'],
                    }
            server_futures = [
                handle_server(user, server_name, server)
                for server_name, server in servers.items()
            ]
            results = yield multi(server_futures)
            if not cull_users:
                return
            # some servers are still running, cannot cull users
            still_alive = len(results) - sum(results)
            if still_alive:
                app_log.debug(
                    "Not culling user %s with %i servers still alive",
                    user['name'],
                    still_alive,
                )
                return False

            should_cull = False
            if user.get('created'):
                age = now - parse_date(user['created'])
            else:
                # created may be undefined on jupyterhub < 0.9
                age = None

            # check last activity
            # last_activity can be None in 0.9
            if user['last_activity']:
                inactive = now - parse_date(user['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'created' field which is never None
                inactive = age

            should_cull = (
                inactive is not None and inactive.total_seconds() >= inactive_limit
            )
            if should_cull:
                app_log.info("Culling user %s (inactive for %s)", user['name'], inactive)

            if max_age and not should_cull:
                # only check created if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling user %s (age: %s, inactive for %s)",
                        user['name'],
                        format_td(age),
                        format_td(inactive),
                    )
                    should_cull = True

            if not should_cull:
                app_log.debug(
                    "Not culling user %s (created: %s, last active: %s)",
                    user['name'],
                    format_td(age),
                    format_td(inactive),
                )
                return False

            req = HTTPRequest(
                url=url + '/users/%s' % user['name'], method='DELETE', headers=auth_header
            )
            yield fetch(req)
            return True

        for user in users:
            futures.append((user['name'], handle_user(user)))

        for (name, f) in futures:
            try:
                result = yield f
            except Exception:
                app_log.exception("Error processing %s", name)
            else:
                if result:
                    app_log.debug("Finished culling %s", name)


    if __name__ == '__main__':
        define(
            'url',
            default=os.environ.get('JUPYTERHUB_API_URL'),
            help="The JupyterHub API URL",
        )
        define('timeout', default=600, help="The idle timeout (in seconds)")
        define(
            'cull_every',
            default=0,
            help="The interval (in seconds) for checking for idle servers to cull",
        )
        define(
            'max_age',
            default=0,
            help="The maximum age (in seconds) of servers that should be culled even if they are active",
        )
        define(
            'cull_users',
            default=False,
            help="""Cull users in addition to servers.
                    This is for use in temporary-user cases such as tmpnb.""",
        )
        define(
            'concurrency',
            default=10,
            help="""Limit the number of concurrent requests made to the Hub.
                    Deleting a lot of users at the same time can slow down the Hub,
                    so limit the number of API requests we have outstanding at any given time.
                    """,
        )

        parse_command_line()
        if not options.cull_every:
            options.cull_every = options.timeout // 2
        api_token = os.environ['JUPYTERHUB_API_TOKEN']

        try:
            AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
        except ImportError as e:
            app_log.warning(
                "Could not load pycurl: %s\n"
                "pycurl is recommended if you have a large number of users.",
                e,
            )

        loop = IOLoop.current()
        cull = partial(
            cull_idle,
            url=options.url,
            api_token=api_token,
            inactive_limit=options.timeout,
            cull_users=options.cull_users,
            max_age=options.max_age,
            concurrency=options.concurrency,
        )
        # schedule first cull immediately
        # because PeriodicCallback doesn't start until the end of the first interval
        loop.add_callback(cull)
        # schedule periodic cull
        pc = PeriodicCallback(cull, 1e3 * options.cull_every)
        pc.start()
        try:
            loop.start()
        except KeyboardInterrupt:
            pass
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jupyterhub-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: jupyterhub-role
rules:
  - apiGroups: [""]
    resources: ["*"]
    verbs: ["get", "list", "watch", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: jupyterhub-rb
subjects:
  - kind: ServiceAccount
    name: jupyterhub-sa
roleRef:
  kind: Role
  name: jupyterhub-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jupyteruser-sa
---
# PVC for Notebooks
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: notebooks-pv-claim
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jupyterhub
  labels:
    application: jupyterhub
spec:
  selector:
    matchLabels:
      application: jupyterhub
  replicas: 1
  template:
    metadata:
      labels:
        application: jupyterhub
    spec:
      serviceAccountName: jupyterhub-sa
      containers:
        - name: jupyterhub
          image: labshare/jupyterhub:0.3.2
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
              # This name will be used in the Service.
              name: jupyter-http
            - containerPort: 8081
              # This name will be used in the Service.
              name: jupyter-in
          command: [
            "jupyterhub",
            "-f",
            "/srv/jupyterhub/config/jupyterhub-config.py",
            "--debug"
          ]
          volumeMounts:
            - mountPath: /srv/jupyterhub/config/jupyterhub-config.py
              name: jupyterhub-config
              subPath: jupyterhub-config.py
            - mountPath: /srv/jupyterhub/config/cull-idle-servers.py
              name: cull-idle-servers
              subPath: cull-idle-servers.py
      volumes:
        - name: jupyterhub-config
          configMap:
              name: jupyterhub-config
              items:
              - key: jupyterhub-config.py
                path: jupyterhub-config.py
        - name: cull-idle-servers
          configMap:
              name: cull-idle-servers
              items:
              - key: cull-idle-servers.py
                path: cull-idle-servers.py
---
# JupyterHub services
# External JupyterHub UI Service definition
apiVersion: v1
kind: Service
metadata:
  name: jupyterhub
  namespace: default
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 32003
    name: http
    targetPort: jupyter-http
  selector:
    application: jupyterhub
---
# Internal JupyterHub API Service definition
apiVersion: v1
kind: Service
metadata:
  name: jupyterhub-internal
spec:
  ports:
  - port: 8081
    name: http
    # Use named container port.
    targetPort: jupyter-in
  selector:
    application: jupyterhub
---
# Plots Backend Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vision-api
  labels: 
    app: vision-api
spec:
  selector:
    matchLabels:
      app: vision-api
  replicas: 1
  template: 
    metadata:
      labels:
        app: vision-api
    spec:
      containers:
      - name: vision-api
        image: labshare/vision-api:0.4.0
        imagePullPolicy: Always
        ports:
        - containerPort: 3000
          name: vision-api
        env:
        - name: COLLECTION_PATH
          value: "/data/wipp/csv-collections/<id>"
        - name: DATASET_BASE_PATH
          value: "/data/csv/"
        - name: ELASTIC_APM_SERVICE_NAME
          value: vision-api
        volumeMounts:
        - name: shared-data
          mountPath: /data/wipp
      volumes:
      - name: shared-data
        persistentVolumeClaim:
          claimName: "wipp-pv-claim"
---
# Plots Backend Service
apiVersion: v1
kind: Service
metadata:
  name: vision-api
spec:
  type: NodePort
  ports: 
  - port: 80
    nodePort: 32011
    targetPort: 3000
  selector:
    app: vision-api
---
# Plots Frontend Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vision-ui
  labels: 
    app: vision-ui
spec:
  selector:
    matchLabels:
      app: vision-ui
  replicas: 1
  template: 
    metadata:
      labels:
        app: vision-ui
    spec:
      containers:
      - name: vision-ui
        image: labshare/vision-ui:0.3.0
        imagePullPolicy: Always
        ports:
        - containerPort: 80
          name: nginx
        env:
        - name: VISION_API_URL
          value: "http://localhost:32011/"
---
# Plots Frontend Service
apiVersion: v1
kind: Service
metadata:
  name: vision-ui
spec:
  type: NodePort
  ports: 
  - port: 80
    nodePort: 32004
    targetPort: nginx
  selector:
    app: vision-ui
---
# WIPP-Tensorboard Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wipp-tensorboard
spec:
  selector:
    matchLabels:
      app: wipp-tensorboard
  replicas: 1
  template:
    metadata:
      labels:
        app: wipp-tensorboard
    spec:
      containers:
      - image: tensorflow/tensorflow:2.0.0b1-py3
        name: wipp-tensorboard
        command: ["tensorboard"]
        args: ["--logdir=/data/WIPP-plugins/tensorboard-logs"]
        volumeMounts:
        - mountPath: /data/WIPP-plugins
          name: data
          readOnly: true
        ports:
        - containerPort: 6006
        resources: {}
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: wipp-pv-claim
      restartPolicy: Always
---
# WIPP-Tensorboard Service
apiVersion: v1
kind: Service
metadata:
  name: wipp-tensorboard
spec:
  type: NodePort
  ports:
  - name: "6006"
    port: 6006
    targetPort: 6006
    nodePort: 32005
  selector:
    app: wipp-tensorboard
---
### Run a job to fix permissions to allow integration with Notebooks
apiVersion: batch/v1
kind: Job
metadata:
  name: fix-permissions
spec:
  template:
    spec:
      containers:
      - name: fix-wipp-permissions
        image: busybox
        command: [ "sh", "-c", "chown 1000:1000 /data/WIPP-plugins" ]
        volumeMounts:
          - name: volume
            mountPath: /data/WIPP-plugins
      volumes:
      - name: volume
        persistentVolumeClaim:
          claimName: wipp-pv-claim
      restartPolicy: Never
  backoffLimit: 4
---
